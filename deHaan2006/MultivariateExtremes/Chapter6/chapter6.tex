%!TEX program =pdflatex
\documentclass{beamer}
\usetheme{CambridgeUS}
%%define new comand
\def\argmin{\mathop{\rm arg~min}\limits}
\def\argmin{\mathop{\rm arg~min}\limits}
\newcommand{\bdelta}{\boldsymbol{\delta}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}
\newcommand{\brho}{\displaystyle{\large{\boldsymbol{\rho}}}}
\newcommand{\bgamma}{\boldsymbol{\gamma}}
\newcommand{\bfeta}{\boldsymbol{\eta}}
\newcommand{\bPsi}{\boldsymbol{\Psi}}
\newcommand{\bmu}{\boldsymbol{\mu}}
\newcommand{\bvartheta}{\boldsymbol{\vartheta}}
\newcommand{\bzero}{\mathbf{0}}
\newcommand{\bone}{\mathbf{1}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bD}{\mathbf{D}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\suit}[1]{\left(#1\right)}
\newcommand{\abs}[1]{\left\vert#1\right\vert}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\msuit}[1]{\left[ #1 \right]}
\author{de Haan and Ferreira(2006)}
\title{Chapter 6}
\date{}
\begin{document}
\begin{frame}
\titlepage
\end{frame}

\begin{frame}
    \frametitle{EVT: biviraite case}
Suppose $(X_1.Y_1), (X_2,Y_2),\dots$ be i.i.d. random vectors with distribution function $F$. Suppose that there exist sequences of constants $a_n,c_n>0, b_n,d_n \in \mathbb{R}$ a distribution function $G$ with non -degenerate marginals such that for all continuity points $(x,y)$ of $G$,
\begin{equation}\tag{6.1.1}
    \begin{aligned}
        &\lim_{n \to \infty}P(\dfrac{\max\suit{X_1,X_2,\dots,X_n}-b_n}{a_n}\le x,\dfrac{\max\suit{Y_1,Y_2,\dots,Y_n}-d_n}{c_n}\le y)   \\
        &\quad\quad =G(x,y).
    \end{aligned}
\end{equation}

Any limit distribution function $G$ in (6.1.1) with non-degenerate marginals is called a multivariate extreme value distribution.
\end{frame}


\begin{frame}
    \frametitle{EVT: biviraite case}
Since (6.1.1) implies convergence of the one-dimensional two marginal distribution, we have 
$$
\lim_{n \to \infty}P\suit{\dfrac{\max\suit{X_1,X_2,\dots,X_n}-b_n}{a_n}\le x}=G(x,\infty) ,
$$
and 
$$
\lim_{n \to \infty}P\suit{\dfrac{\max\suit{Y_1,Y_2,\dots,Y_n}-d_n}{c_n}\le y}   =G(\infty,y).
$$

\end{frame}

\begin{frame}
    \frametitle{EVT: bivirate case}
Let $F_1,F_2$ denote the marginal distribution of $F$. 

\bigskip
Define $U_i(t):=F_i^{\leftarrow}(1-1/t), i=1,2.$ Then there exist constants $a_n,b_n,c_n,d_n$ such that
\begin{equation}
    \begin{aligned}
        \lim_{t\to \infty} \dfrac{U_1(nx)-b_n}{a_n} & =\dfrac{x^{\gamma}_1-1}{\gamma_1},      \\
        \lim_{t\to \infty} \dfrac{U_2(nx)-d_n}{c_n} & =\dfrac{x^{\gamma}_2-1}{\gamma_2}.  
    \end{aligned}
\end{equation}
\end{frame}

\begin{frame}
    \frametitle{EVT: bivirate case}
    Now, we return to (6.1.1), which can be written as 
\begin{equation}\tag{6.1.8}
    \lim_{n \to \infty} F^n (a_n x+b_n, c_ny+d_n)=G(x,y).
\end{equation}
 
If $x_n \to u, y_n \to v$, then 
\begin{equation}\tag{6.1.9}
    \lim_{n \to \infty} F^n (a_n x_n+b_n, c_ny_n+d_n)=G(u,v).
\end{equation}
Apply (6.1.9) with 
$$
x_n=\dfrac{U_1(nx)-b_n}{a_n},y_n=\dfrac{U_2(ny)-d_n}{c_n}
$$
then 
$$
\lim_{n\to \infty} F^n (U_1(nx),U_2(ny))=G\suit{\dfrac{x^{\gamma}-1}{\gamma},\dfrac{y^{\gamma}-1}{\gamma}}:=G_0(x,y)
$$
\end{frame}


\begin{frame}
    \frametitle{}
\begin{itemize}
    \item     The marginal distribution of function $G_0$ is    $\exp(-x^{-1})$ and $\exp(-y^{-1})$.
    \bigskip
    \item The marginal distribution does not depend on $\gamma$ and other parameters.
    \bigskip
    \item Now, we can only consider the dependence structure.
\end{itemize}

    

\end{frame}

\begin{frame}
    \frametitle{Corollary 6.1.3}
For any $(x,y)$ for which $0<G_0(x,y)<1$,
\begin{equation}\tag{6.1.11 }
    \lim_{n\to \infty} n\set{1-F(U_1(nx),U_2(ny))}=-\log G_0(x,y)
\end{equation}
This also holds by replacing $n$ by $t$, where $t$ runs through the real numbers.

\end{frame}

\begin{frame}
    \frametitle{Exponent Measure}
There are set functions $v,v_1,v_2$ defined for all Borel sets $A\subset \mathbb{R}_{+}^2$ with 
$$
\inf_{(x,y)\in A}\max(x,y)>0 
$$
such that

1.
$$
\begin{aligned}
    v_n\set{(s,t)\in \mathbb{R}_{+}^2: s>x \ or \ t>y}&=n\suit{1-F(U_1(nx),U_2(ny))},\\
    v\set{(s,t)\in \mathbb{R}_{+}^2: s>x\  or \ t>y}&=-\log G_0(x,y)
\end{aligned}
$$
2. for all $a>0$ the set functions $v,v_1,v_2,\dots$ are finite measures on $\mathbb{R}_{+}^2 \ [0,a]^2$

3.for  each Borel set $ A\subset \mathbb{R}_{+}^2$ with $\inf_{(x,y)\in A}\max(x,y)>0$ and $v(\partial A)=0$,
$$
\lim_{n \to \infty} v_n(A)=v(A).
$$

The measure $v$ is sometimes called the exponent measure of the extreme value distribution $G_0$.
\end{frame}

\begin{frame}
    \frametitle{Homogeneity of $v$}
For any Borel set $A\subset \mathbb{R}_{+}^2$,with $\inf_{(x,y)\in A}\max(x,y)>0$ and $v(\partial A)=0$,
$$
v(aA)=a^{-1}v(A)
$$

\begin{proof}
    It is easy to verify this property by Corollary 6.1.3.
\end{proof}
\end{frame}


\begin{frame}
    \frametitle{The Spectral Measure}
    The homogeneity property of the exponent measure $v$ suggests a coordinate transformation in order to capitalize on that. 

Examples are
$$
\left\{
\begin{array}{l}
    r(x,y)=\sqrt{x^2+y^2}\\
    d(x,y)=\arctan \frac{y}{x}
\end{array}   
\right.
$$
$$
\left\{
\begin{array}{l}
    r(x,y)=x+y\\
    d(x,y)=\frac{x}{x+y}
\end{array}   
\right.
$$
$$
\left\{
\begin{array}{l}
    r(x,y)=x \lor y\\
    d(x,y)=\arctan \frac{x}{y}
\end{array}   
\right.
$$

\end{frame}


\begin{frame}
    \frametitle{The Spectral Measure}
    Let  us  start  with  the first transformation.  Define  for  constants $r>0$ and $\theta \in [0,\pi/2]$ the set
    $$
B_{r,\theta}=\set{(x,y)\in \mathbb{R}_{+}^{2*}: \sqrt{x^2+y^2}>r \ and \ \arctan \frac{y}{x}\le \theta}
    $$
    Clearly $B_{r,\theta}=rB_{1,\theta}$ and hence 
    $$
v(B_{r,\theta}):=r^{-1} v(B_{1,\theta}).
    $$
Set for $0\le \theta \le \pi/2$,
$$
\Psi(\theta):=v(B_{1,\theta}).
$$
\end{frame}

\begin{frame}
    \frametitle{The Spectral Measure}
Write $s=r\cos \theta, t=r\sin \theta$. Take $x,y>0$,
$$
\begin{aligned}
    -\log G_0(x,y) & = v\set{(s,t): s>x \ or \ t>y}\\
                & = v\set{(r,\theta):r\cos \theta>x \ or \ r\sin \theta >y} \\
                & = v \set{(r,\theta): r> \frac{x}{\cos\theta} \land \frac{y}{\sin \theta}} \\
                & =\int_{x/\cos\theta< y/\sin \theta} \int_{r>x/\cos\theta} \frac{\partial^2 v(B(r,\theta))}{\partial r \partial \theta} dr d\theta+ \\
                &\quad + \int_{x/\cos\theta> y/\sin \theta} \int_{r>y/\sin \theta} \frac{\partial^2 v(B(r,\theta))}{\partial r \partial \theta} dr d\theta\\
                & =\int_{x/\cos\theta< y/\sin \theta} \int_{r>x/\cos\theta} \frac{dr}{r^2}\frac{\partial v(B_{1,\theta})}{\partial \theta}d\theta\\
                &\quad +\int_{x/\cos\theta> y/\sin \theta} \int_{r>y/\sin \theta}  \frac{dr}{r^2}\frac{\partial v(B_{1,\theta})}{\partial \theta}d\theta
\end{aligned}
  $$  
\end{frame}

\begin{frame}
    \frametitle{Theorem 6.1.14}
There exist a finite measure on $[0,\pi]$ such that for $x,y>0$,
$$
\begin{aligned}
    G_0(x,y)=\exp\suit{-\int_0^{\pi/2}\suit{\frac{\cos \theta}{x}\lor \frac{\sin \theta}{y}}\Psi(d \theta)}
\end{aligned}
$$
with the side functions
$$
\int_{0}^{\pi/2}\cos \theta \Psi(d\theta)=\int_{0}^{\pi/2}\sin \theta \Psi(d\theta)=1.
$$
    

\end{frame}



\begin{frame}
    \frametitle{Copula}
We could also describe the dependence using copulas.
\bigskip
\begin{itemize}
    \item If $F$ is the distribution function of the random vector $(X,Y)$, the copula $C$ associated with $F$ is a distribution function that satisfies $F(x,y)=C(F_1(x),F_2(y))$.
    \item It contains complete information about the joint distribution of $F$ apart from the marginal distribution.
\end{itemize}

$$
\begin{aligned}
    F(F_1^{-1}(x),F_2^{-1}(y))& =P(X\le F_1^{x}, Y\le F_2^{-1}(y))\\
    &=P(F_1(X)\le x, F_2(y)\le y)\\
    &:=C(x,y)
\end{aligned}
$$
Then, $F(x,y)=C(F_1(x),F_2(y))$.
\end{frame}


\begin{frame}
    \frametitle{L function}
Define for $0<x,y<1$, 
$$
C(x,y):=G_0(-1/\log x, -1/\log y).
$$
Then, $C$ is a copula and the homogeneity of the exponent measure implies that: for $0<x,y<1$, $a>0$,
$$
C(x^a,y^a)=C^a(x,y).
$$

This relation is not very tractable for analysis, we instead consider the $L$ function defined by 
$$
\begin{aligned}
    L(x,y)&:=-\log G_0(1/x,1/y)\\
    &=v\set{(s,t)\in \mathbb{R}_{+}^2:s>1/x \ or \ t>1/y}.
\end{aligned}
$$
\end{frame}


\begin{frame}
    \frametitle{Properties of $L$ function}

\begin{enumerate}
    \item $L(ax.ay)=aL(x,y)$, for all $a,x,y>0$
    \item $L(x,0)=L(0,x)=x$, for all $x>0$
    \item $x\lor y \le L(x+y)\le x+y$
    \item If $X,Y$ are independent, then $L(x,y)=x+y$. If $X,Y$ are completely positive dependent, then $L(x,y)=x\lor y$.
    \item $L$ is continuous.
    \item $L(x,y)$ is a convex function.
\end{enumerate}
 


\end{frame}


\begin{frame}
    \frametitle{Other Measures of dependence}
Define the set $Q_c$ by 
$$
Q_c:=\set{(x,y)\in\mathbb{R}_{+}^2:-\log G_0(1/x,1/y)\le c}
$$
The function $R$ is defined as 
$$
R(x,y)=x+y-L(x,y)
$$
The function $\chi$ is defined as 
$$
\chi(t)=-R(t,1)
$$

The function $A$ is defined as 
$$
A(t):=L(1-t,t)
$$

\end{frame}



\begin{frame}
    \frametitle{Domains of Attractions}
\begin{itemize}
    \item We have now discussed the multivariate extreme value distribution $G$.
    \bigskip
    \item Now, we are going to discuss which $F$ belongs to the max domain of attractions of multivariate extreme value distribution.
\end{itemize}
\end{frame}


\begin{frame}
    \frametitle{Theorem 6.2.1}
If $F$ belongs to the maximum domain of attraction, the followings are equivalent.

a. 
\begin{equation}\tag{6.2.1}
    \lim_{t\to \infty} \dfrac{1-F(U_1(tx),U_2(ty))}{1-F(U_1(t),U_2(t))}=S(x,y)  
\end{equation}

with $S(x,y)=\log G((x^{\gamma_1}-1)/\gamma,(y^{\gamma_2}-1)/\gamma)/\log G(0,0)$.

b. For all $r>1$ and all $\theta \in [0,\pi/2]$ that   are continuity point of $\Psi$,

\begin{equation}\tag{6.2.1}
P\suit{V^2+W^2>t^2r^2\ and \frac{W}{V}\le \tan \theta | V^2+W^2>t^2}\to r^{-1}\frac{\Psi(\theta)}{\Psi(\pi/2)},
\end{equation}
where $V=1/(1-F_1(X)), W=1/(1-F_2(Y))$.
\bigskip

Conversely, if the continuous  marginal distribution function  $F_i$ are in the domain of attraction of univariate extreme value distribution and any limit relation (6.1.1) or (6.1.2) holds, then $F$ is in the domain of attraction of $G$.

\end{frame}




\begin{frame}
    \frametitle{Asymptotic Independence}

    Let $(X_1,\dots,X_d)$  be a random vector with distribution function  $F$. Let the marginal distribution $F_i$ satisfies the domain of attraction condition. If
    $$
    \dfrac{P(X_i>U_i(t),X_j>U_j(t))}{P(X_i>U_i(t))}\to 0
    $$
    for all $1\le i<j\le d$, then 
    $$
\lim_{n \to \infty} F^n(a_n^{(1)}x_1+b_n^{(1)},\cdots, a_n^{(d)}x_1+b_n^{(d)})=\exp\suit{-\sum_{i=1}^d (1+\gamma_ix_i)^{-1/\gamma_i}}.
    $$

\end{frame}
\end{document}